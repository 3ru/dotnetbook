# Домены приложений

Самое время пришло поговорить про домены приложений (AppDomains). Причем если вы скажете, что эта тема достаточно элементарна и понятна чтобы позволить себе пропустить данную главу, то в её защиту скажу что тема эта кажется такой простой по одной причине: её зачем-то обходят стороной тогда как если вглядеться поглуже, выяснится что она очень интересна и многогранна.

Начнем мы наше изучение с несколько философского вопроса: зачем вообще вводили домены приложений? Для ответа на этот вопрос необходимо немного окунуться в историю: когда миром правил С++ и технология COM, а программисты только прощупывали почву чтобы изобрести что-то наподобие нашей любимой платформы. Работа приложений в те времена по-большей части была построена следующим образом: если некоторое программа позволяет расширять собственный функционал, то оно должно выставить некоторые стандартизированные методы наружу: это либо банальный экспорт методов либо экспорт объектно-ориентированного API через COM. Однако, поскольку мы в любом случае имеем дело с общей для обоих участников процесса памятью (память линейна для процесса, процессы изолированы друг от друга), то и программа и код, ее расширяющий имеют общий доступ к памяти друг друга. Как следствие этого - между ними изоляции нет абсолютно никакой. А потому, правильным образом пошаманив с указателями можно легко и просто прочитать память хоста, находясь в плагине.

Это вызывает множество проблем с безопасностью: если само приложение можно сделать безопасным с точки зрения проникновения, то расширив его функционал сторонним кодом такая уверенность сходит на нет: теперь вы живете с мыслью, что пользователь в поисках взломанной версии дорогого плагина отправится скачивать его со стороннего ресурса и скачает некий вылеченый от жадности плагин к вашему приложению чтобы запустить его. Конечно же, бесплатный сыр бывает только в мышеловке: установив плагин и запустив приложение такой пользователь ничего не заметит, но внедренный авторами сайта, с которого скачан плагин код начнет изучать ваш компьютер в поисках наживы. Или просто заснет до лучших времен, когда такая нажива на вашем компьютере появится.

Одним из решений подобной проблемы может стать запуск плагинов в отдельных процессах: это будет идеальной изоляцией памяти хоста от его расширений на уровне железа: процессы изолируются операционной системой в паре с процессором. Однако, это накладывает определенные ограничения: скорость заметно снижается на всех этапах взаимодействия. Ведь при вызове методов API вы должны:

  - Сериализовать данные (параметры метода)
  - Осуществить передачу до получателя через кросспроцессный механизм:
    - Например, через сокет
    - Или Shared Memory
  - На другом конце - десериализовать
  - Вызвать метод

Т.е. нам необходимо выполнить **очень** много действий. Однако плата хоть и велика, результат будет прекрасен: при вызове метода гарантируется полная изоляция между хостом и плагинами. Плюс ко всему поскольку протокол контролируете именно вы, вы контролируете все возможности по переполнению буферов, которые зачастую используются при взломах.

Теория звучит великолепно если бы не одно "но": необходимы дополнительные средства автоматизации кодогенерации библиотеки для клиента и хоста по некоторому файлу с описанием интерфейса API. Иначе велик риск человеческого фактора при ручной реализации такой обертки. Таких средств на горизонте не наблюдалось по одной простой причине: пока мы боремся за изоляцию памяти никто не мешает процессу плагина что-либо записать на жесткий диск или же обратиться по сети к какому-либо сайту. Другими словами, создав изоляцию по памяти, мы поставили дверь, вокруг которой нет ни одной стены. Согласитесь, тут мы очень похожи на кошку, которая спряталась за шторой одной только головой. Что делать в такой ситуации? Оборачивать весь WinAPI своими обертками? Это возможно: чтобы получить доступ к WinAPI вы импортируете dll библиотеки и можете переопределить точки входа в методы, заменив их на свои и возвращать ошибки, когда плагин начнет работать с файлами, с которыми работать не должен. Звучит практично, безопасно, элегантно. Если бы не одно но: код плагина имеет доступ к собственной памяти, а его программист - умеет пользоваться отладчиком и легко сможет понять, где ему необходимо искать таблицу расположения реальных адресов методов WinAPI.

Всё это звучит как безнадёга. Однако, в данном разговоре не хватает понимания: что конкретно мы защищаем и от чего (а главное, почему?) мы защищаемся. Защитить мы пытаемся наши данные, которые могут содержать некие пароли, средства доступа к нашим финансам и личным фотоархивам. Скрывать всегда есть что, однако скрываем мы всегда данные. От чего мы их скрываем? От неких действий. Если нет действий, значит данные находятся в безопасности. Значит, если обзавестись средством контроля над тем, что делает (какой исполняет код) наш плагин, значит мы получим средство обеспечения безопасности наших данных не прибегая к созданию сложной системы взаимодействия между процессами. А как достичь полного контроля? Мы же не станем анализировать реальный скомпилированный код: средств обмана анализаторов и прочих антивирусов предостаточно. Идеально - это когда мы сами осуществляем компиляцию приложения. Этой задачей занимается JIT: он просто не даст скомпилировать что-либо опасное. Второе - необходимо иметь уровни доверия: ведь в конце концов само приложение может иметь средства сборки опасного кода. Запрещать опасные участки необходимо плагинам и прочим средствам расширения: им-то мы не доверяем. И тут мы вплотную подошли к теме нашего разговора: JIT и домены приложений.

Поскольку наш код и код, которому мы по каким-то причинам не доверяем порождатся единым компилятором (JIT), который собираем приложение относительно MSIL, то компилятор этот может быть настроен таким образом, чтобы не собирать код, который будет признан опасным. Имея единственный компилятор мы получаем средство тотального контроля над кодом, который исполняется процессором. Осталось получить абстракцию, в рамках которой будут существовать некие жёсткие правила: это и есть домены приложений.

Если попытаться обобщить, то домен приложения - это изолированная область исполнения кода. Вы можете настроить любые правила исполнения: от доступа к рефлексии до доступа к жесткому диску. И поскольку все базовые методы доступа к этим ресурсам находятся во владении BCL и CLR, то сторонний код, загруженный в хорошо настроенную среду не сможет выбраться из такой песочницы.

Дополнительно, хотелось бы отметить что в понимании иерархии компонентов может сложиться не совсем верная картина мира: что базовым элементом системы типов является сам тип, далее - находятся пространства имен, модули и сборки, которые их объединяют. И венцом иерархии является домен приложения. А на самом деле всё совсем не так. Видимая иерархия является переплетением различных слоев фреймворка .NET: если типы и пространства имен образуют систему типов и кода (мысленно расположим их по центру), то модули (слева) - средство их объединения в единую группу **в вопросе хранения**. Они не оказывают никакого влияния с точки зрения смыслового объединения. Сборка при этом - также средство хранения и объединяет модули в единый файл. Домен же занимает совершенно обособленную позицию: справа. Ведь это ни средство хренения ни средство логического разделения: это средство исполнения кода, в него загруженного.

## Изоляция по памяти

Как я уже говорил, нам нет никакой необходимости делать изоляцию по памяти: т.к. контроль над исполнением кода (путем его самостоятельной компиляции) дает домену все возможности для контроля всего приложения. Как это достигается? Домен - это некий изолирующий контейнер. Потому, если мы имеем дело с загружемыми библиотеками, было бы логично если бы эти библиотеки загружались бы не в общее для всех место, а в эту песочницу. Теперь смотрите: если в таком раскладе мы имеем один и тот же тип:

```csharp
class SomeType
{
    public int X { get; set; }
}
```

Который мы попробуем загрузить в два домена, то мы получим дважды скомпилированный код. Что это будет означать? Что у экземпляров этих типов адрес таблицы виртуальных методов совпадать не будет:

```csharp
// В первом домене:
var x = new SomeType();
var handle1 = x.GetType().TypeHandle;

// Во втором домене:
var x = new SomeType();
var handle2 = x.GetType().TypeHandle;

// где-то в общем коде
Console.WriteLine(handle1 == handle2);
// --> false
```

Это значит что если мы получим каким-либо образом из второго домена ссылку на объект и в первом домене попробуем привести к типу `SomeHandle`, то ничего не получится: типы не совпадут. Получается, что загрузив код в разные домены вместо загрузки в некое междоменное пространство `CLR` строит первый барьер защиты. Второй барьер защиты: нам необходимо организовать взаимодействие между доменами: быть полностью изолированными - так себе возможности и хочется иметь не полную изоляцию, а контролируемое взаимодействие. Интереснее получать отдавать некие данные в плагин для неких расчетов или действий и получать результаты работы - назад. Это можно организовать при помощи прокси-методов. Посмотрим на пример такого вызова с использованием псевдокода:

```csharp
// Мы получаем ссылку на объект из второго домена в первом
var objectFromSecondDomain = GetObjectFromDomain(domain2);

// И пробуем вызвать метод (как мы это обычно делаем)
var data = objectFromSecondDomain.Method();

// Что происходит на самом деле:
// Вместо метода вызывается невидимый для нас прокси метод,
// объявленный где-то в CLR, а не в самом типе:
var data = objectFromSecondDomain.MethodToDomainProxy();
private SomeType MethodToDomainProxy()
{
  if(CurrentDomain == Domain2){
    return objectFromSecondDomain.Method();
  }

  // Который приводит ссылку к необходимому типу из второго домена
  var castedType = (SomeType_FromDomain2)(objectFromSecondDomain);
  // сохраняем ссылку на наш домен и выставляем Domain 2 как активный
  var priorDomain = CurrentDomain;
  CurrentDomain = Domain2;
  // вызывает метод
  var result = castedType.Method();
  // восстанавливаем ссылку на текущий домен
  CurrentDomain = priorDomain;
  // и проверяет объект, который возвращен из метода на безопасность
  if(!CLR.VerifyResultForSafety(result))
  {
      throw CLR.GetExceptionForUnsafeCall();
  }
  // если все хорошо, возвращает результат
  return result;
}
```

Хорошо: теперь мы знаем обо всех вызовах, которые происходят в системе. Однако, это накладывает огромные расходы: нам ведь каждый раз необходимо понимать, что мы вызываем метод именно в другом домене, а не в текущем. А таких вызовов будет большинство: междоменного взаимодействия очень мало. Значит, чтобы такие расходы ложились бы только на необходимо ввести некий специальный тип, при наследовании от которого CLR поймет что он будет участвовать в междоменном взаимодействии и создаст прокси-методы только для него. Тип этот - `MarshalByRefObject` и это решение также создаст еще один барьер безопастности: вы не можете вызывать методы, скомпилированные для других доменов напрямую: только для `MarshalByRefObject` типов.

Что мы имеем на данный момент? Во-первых если наш код загружен в два и более доменов, то скомпилирован он будет соответствующее число раз, а значит с точки зрения типов один и тот же тип, скомпилированных для нескольких доменов - это несколько разных типов. Далее, вводятся проверки для вызовов методов из одного домена в другой. Можно назвать это словом "таможня". При пересечении границы все что проходит через такую вот таможню должно досматриваться: этим действием контролируется что все ограничения соблюдены. Ну и чтобы CLR понимала, какие типы могут участвовать во взаимодействии между доменами проходить границу могут не все типы, а только те, которые унаследовали `MarshalByRefObject`. Достаточно ли этого? Наверное, нет: мы отдаем в метод, скомпилированный для другого домена некую ссылку на свой объект, тот его принимает, но в другом домене работать с таким доменом нет никакой возможности: его система типов отлична и приведение типов работать не будет. Как тогда организовать взаимодействие?

Решения для такой проблематики два. Перовое решение - необходимо сделать так, чтобы передаваемые данные полностью бы отражали содержание объекта, но при этом не были бы самим объектом - раз. И при этом были бы достаточно простыми чтобы не представлять опасности - два. Такими данными язвяются сериализованные данные:

  - При сериализации мы все превращаем в условную строку. Т.е. не можем передать ссылку на объект, обладающий повышенными привелегиями;
  - При десериализации мы можем сформировать объекты только из того что было передано через условную строку: сериализованный объект. Ничего опасного мы оттуда получить не сможем.

Второе решение - организовать средство повышенного доверия к передаваемым между доменами данным. Такого доверия, что можно не волнуясь отдавать без всякой сериализации и десериализации ссылки на объекты в соседний домен и не волноваться что это как-то повлияет на безопасность. Чтобы решить эту задачу, был разработан Shared AppDomain.

Всякая сборка, попавшая в данный домен обладает сверхдоверием: если какому-либо коду из обычных доменов требуются типы из сборок, загруженных Shared AppDomain, то CLR не станет загружать эти сборки по второму разу в ваш домен: для вас будет создан экземпляр типа, таблица виртуальных методов которого смотрит на описание типа в Shared AppDomain.

Это порождает следующий примерный алгоритм проверки параметров метода (псевдокод):

```csharp
private bool CLR.VerifyForSafety(object obj, AppDomain target)
{
    if(obj.GetType().Assembly.AppDominLoadedInto.IsShared)
      return true;

    if(AppDomain.CurrentDomain == target)
      return true;

    return Serialization.CheckSerializable(obj);
}
```

